# Diagnostics

Building from the models used in section 3, we now do a deep dive into the XGBoost algorithm. We choose XGBoost because it performed well in the previous section, it has many tunable hyperparameters that are interesting to explore, and it has performed well in many online prediction competitions (@xgboostpaper). First, let's do an overview of XGBoost and boosting in general. The overview presented here is mainly adapted from the original paper by @xgboostpaper.

The idea behind tree boosting algorithms is to combine simple classification trees and create a final classifier taking into account all these weak classifiers. Boosting is known to prevent overfitting and produce overall better classification accuracy (@boostingintro). Different boosting algorithms differ in the way they incorporate the weak classifiers. For example, the first boosting algorithm called AdaBoost (short for Adaptive Boosting) by @adaboost, combines the classifiers and adaptively reweighs the data so misclassified points get more weight.

XGBoost minimizes a regularized loss function, where the regularization term penalizes both the number of leaves in the tree and the vector of leaf weights. At each step, the algorithm uses a greedy approach to select the tree structure that yields a larger loss reduction. Furthermore, XGBoost incorporates a shrinkage term that scales the weights and reduces the influence of each individual tree. Just like random forests, XGBoost samples features at each split. According to @xgboostpaper, the shrinkage term and feature sampling help prevent overfitting.

Implementation of XGBoost is attainable in R using the `xgboost` package by @xgboost. We now go over some of the tunable parameters in `xgboost`. Keep in mind that the argument names are not the same that are used when fitting XGBoost through the `tidymodels` interface.

+ Number of trees used in the ensemble (`nrounds`).
+ Number of randomly sampled features at each split (`colsample_bynode`).
+ Minimum number of observations for a node to be split further (`min_child_weight`).
+ Maximum depth of trees (`max_depth`).
+ Shrinkage term (`eta`).
+ Reduction in the loss function required to split further (`gamma`).
+ Proportion of the data that is exposed to the fitting routine at each iteration (`subsample`).
+ Iterations without improvement before stopping (`early_stop`).

